# Cohort analysis

To perform a cohort analysis, we can group users based on when they joined: often into weekly or monthly cohorts. This works well when we want to track improvements over time, or when we want to understand trends in user behavior.

:::tip
Learn about the term "cohort" [here](/cohort.html), or create your own cohort analysis with [Userfront](https://userfront.com).
:::

## Cohort analysis example

To better visualize cohort analysis, we can view weekly user cohorts for a website, where we care about the percentage of the users who were active on the site.

#### % of users who were active

![Cohort analysis](https://res.cloudinary.com/component/image/upload/v1585000344/permanent/cohort.png)

In this cohort analysis, we have weekly cohorts; every user that signed up during the week of January 5 is included in that cohort, and the data for that cohort is shown in the first row.

Similarly, every user that signed up during the week of January 12 is included in the 2nd row. Every user that signed up during the week of January 19 is in the 3rd row.

Users from the older cohorts have been around longer (because they signed up earlier), so they have more data. That is why the first row has one more column than the second row, the second row has one more column than the third row, and so on.

## Cohort progress

We can view an individual cohort's progress over time by looking at the row for the cohort. Let's look at the group of users who joined during the week of January 19:

![Cohort analysis row](https://res.cloudinary.com/component/image/upload/v1585000344/permanent/cohort-cohort.png)

To better visualize how active this cohort has been, we can view the same data on a graph:

![Cohort retention](https://res.cloudinary.com/component/image/upload/v1585000343/permanent/cohort-retention-time.png)

Here we see that 100% of users were active initially (because they signed up). During the following week (1 week later), 70% of these users were still active on the site. 2 weeks after signing up, it was 53%, and so on.

It's natural to expect that not every user who signs up will continue using the service. However, we can still plan improvements to a service so that fewer users stop using it.

If we assume that users only sign up if they are somewhat interested in the service, then that means they didn't quite get what they expected, or, in many cases, were too busy or forgot about it after joining. Many of these situations have solutions that will keep the user engaged.

## Comparing cohorts

In order to tell whether we are getting better at engaging users over time, we can compare cohorts to one another.

We compare cohorts by looking at the vertical columns, since they represent the same thing for each cohort. When we view column 1 of our example, we can see week 1 retention for all of our cohorts:

![Cohort analysis column](https://res.cloudinary.com/component/image/upload/v1585000343/permanent/cohort-week-1.png)

Each number represents the percentage of users who were still active 1 week after signing up. We can view the same data in a bar graph:

![First week retention](https://res.cloudinary.com/component/image/upload/v1585000343/permanent/cohort-retention-progress.png)

In this case, retention has improved significantly over the course of 7 weeks. While 60% of users were continuing with the service at the beginning of January, by late February that number rose to 90%.

This particular example would tell us that we're getting better: users who joined later are a lot more likely to keep using the service over time. If we didn't see any improvement here, we might want to rethink our focus.

## Actions to improve

In the example above, cohort analysis showed us the improvement over time for a website trying to retain its users. Without any action, it's unlikely that the service would magically improve, but there are concrete steps we could try in order to improve retention.

We could work to address dropoff a number of different ways:

- A welcome email with useful tips
- Better onboarding materials
- Drip campaign emails to highlight product features
- Sending a reminder message after a period of inactivity
- Targeting "better" users to begin with

By trying some of these actions and measuring each week whether or not the actions made a difference, we can both improve the service and better understand our users' needs. Sometimes one improvement has a major effect, but it's often a combination of smaller steps that make an impact.

Without cohort analysis, it can be hard to tell which changes are improving the service, and which are simply taking up time, or even making it worse.

Adding cohort analysis to your growth toolkit helps you to multiply your efforts and continue building momentum. It's these types of actions, applied over time, that lead to rapid growth and a better overall service.
